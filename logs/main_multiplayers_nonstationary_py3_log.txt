Reading argument from command line, importing the configuration from arg = configuration_multiplayers_nonstationary (module = configuration_multiplayers_nonstationary)...

Using Upsilon_T = 4 break-points (time when at least one arm changes), and C_T = 4 change-points (number of changes of all arms).
For this problem, we compute the Delta^change and Delta^opt...
min_change_on_mean = 0.3
min_optimality_gap = 0.09999999999999998
DELTA_for_MUCB = 0.1
EPSILON_for_CUSUM = 0.1
Warning: using the default value for the GAP (Bayesian environment maybe?)
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 1, and class #2<CentralizedMultiplePlay(UCB)> ...
 - One new child, of index 0, and class #1<CentralizedMultiplePlay(kl-UCB)> ...
 - One new child, of index 1, and class #2<CentralizedMultiplePlay(kl-UCB)> ...
Loaded experiments configuration from 'configuration_multiplayers_nonstationary.py' :
configuration = {'horizon': 5000, 'repetitions': 40, 'n_jobs': -1, 'verbosity': 6, 'collisionModel': <function onlyUniqUserGetsReward at 0x7fa707e317b8>, 'finalRanksOnAverage': True, 'averageOn': 0.001, 'plot_lowerbounds': True, 'environment': [{'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}}], 'nb_break_points': 4, 'successive_players': [[rhoRand(UCB), rhoRand(UCB)], [rhoRand(kl-UCB), rhoRand(kl-UCB)], [RandTopM(UCB), RandTopM(UCB)], [RandTopM(kl-UCB), RandTopM(kl-UCB)], [MCTopM(UCB), MCTopM(UCB)], [MCTopM(kl-UCB), MCTopM(kl-UCB)], [Selfish(UCB), Selfish(UCB)], [Selfish(kl-UCB), Selfish(kl-UCB)], [CentralizedMultiplePlay(UCB), CentralizedMultiplePlay(UCB)], [CentralizedMultiplePlay(kl-UCB), CentralizedMultiplePlay(kl-UCB)], [<Policies.MusicalChair.MusicalChair object at 0x7fa702076a90>, <Policies.MusicalChair.MusicalChair object at 0x7fa702076ac8>], [<Policies.MusicalChair.MusicalChair object at 0x7fa702076b00>, <Policies.MusicalChair.MusicalChair object at 0x7fa702076b38>], [<Policies.MusicalChair.MusicalChair object at 0x7fa702076b70>, <Policies.MusicalChair.MusicalChair object at 0x7fa702076ba8>], [<Policies.SIC_MMAB.SIC_MMAB object at 0x7fa702076be0>, <Policies.SIC_MMAB.SIC_MMAB object at 0x7fa702076c18>], [<Policies.SIC_MMAB.SIC_MMAB_UCB object at 0x7fa702076c50>, <Policies.SIC_MMAB.SIC_MMAB_UCB object at 0x7fa702076c88>], [<Policies.SIC_MMAB.SIC_MMAB_klUCB object at 0x7fa702076cc0>, <Policies.SIC_MMAB.SIC_MMAB_klUCB object at 0x7fa702076d30>]], 'players': [Selfish(DiscountedThompson($\gamma=0.95$)), Selfish(DiscountedThompson($\gamma=0.95$))]}
====> TURNING DEBUG MODE ON <=====
plots/ is already a directory here...
Number of players in the multi-players game: 2
Time horizon: 5000
Number of repetitions: 40
Sampling rate for plotting, delta_t_plot: 1
Number of jobs for parallelization: -1
Using collision model onlyUniqUserGetsReward (function <function onlyUniqUserGetsReward at 0x7fa707e317b8>).
More details:
 Simple collision model where only the players alone on one arm samples it and receives the reward.

    - This is the default collision model, cf. [[Multi-Player Bandits Revisited, Lilian Besson and Emilie Kaufmann, 2017]](https://hal.inria.fr/hal-01629733).
    - The numpy array 'choices' is increased according to the number of users who collided (it is NOT binary).
    
Using accurate regrets and last regrets ? True
Using this dictionary to create a new environment:
 {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}}
  Special MAB problem, with arm (possibly) changing at every time step, read from a dictionnary 'configuration' = {'arm_type': <class 'Arms.Bernoulli.Bernoulli'>, 'params': {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}} ...
 - with 'arm_type' = <class 'Arms.Bernoulli.Bernoulli'>
 - with 'params' = {'listOfMeans': [[0.3, 0.5, 0.9], [0.3, 0.2, 0.9], [0.3, 0.2, 0.1], [0.7, 0.2, 0.1], [0.7, 0.5, 0.1]], 'changePoints': [0, 1000, 2000, 3000, 4000]}
 - with 'listOfMeans' = [[0.3 0.5 0.9]
 [0.3 0.2 0.9]
 [0.3 0.2 0.1]
 [0.7 0.2 0.1]
 [0.7 0.5 0.1]]
 - with 'changePoints' = [0, 1000, 2000, 3000, 4000]


 ==> Creating the dynamic arms ...
   - with 'nbArms' = 3
   - with 'arms' = [B(0.3), B(0.5), B(0.9)]
 - Initial draw of 'means' = [0.3 0.5 0.9]
Number of environments to try: 1


Evaluating environment: PieceWiseStationaryMAB(nbArms: 3, arms: [B(0.3), B(0.5), B(0.9)])
- Adding player # 1 = #1<Selfish-DiscountedThompson($\gamma=0.95$)> ...
  Using this already created player 'player' = #1<Selfish-DiscountedThompson($\gamma=0.95$)> ...
- Adding player # 2 = #2<Selfish-DiscountedThompson($\gamma=0.95$)> ...
  Using this already created player 'player' = #2<Selfish-DiscountedThompson($\gamma=0.95$)> ...

New means vector = [0.3 0.5 0.9], at time t = 0 ...

New means vector = [0.3 0.2 0.9], at time t = 1000 ...

New means vector = [0.3 0.2 0.1], at time t = 2000 ...

New means vector = [0.7 0.2 0.1], at time t = 3000 ...

New means vector = [0.7 0.5 0.1], at time t = 4000 ...

Estimated order by the policy #1<Selfish-DiscountedThompson($\gamma=0.95$)> after 5000 steps: [1 2 0] ...
  ==> Optimal arm identification: 66.67% (relative success)...
  ==> Mean distance from optimal ordering: 38.89% (relative success)...

Estimated order by the policy #2<Selfish-DiscountedThompson($\gamma=0.95$)> after 5000 steps: [0 2 1] ...
  ==> Optimal arm identification: 50.00% (relative success)...
  ==> Mean distance from optimal ordering: 61.11% (relative success)...

Giving the final ranks ...

Final ranking for this environment # 0 : Selfish-DiscountedThompson($\gamma=0.95$) ...
- Player # 1 / 2, Selfish-DiscountedThompson($\gamma=0.95$)	was ranked	1 / 2 for this simulation (last rewards = 2222.2).
- Player # 2 / 2, Selfish-DiscountedThompson($\gamma=0.95$)	was ranked	2 / 2 for this simulation (last rewards = 2014.5).

Giving the vector of final regrets ...

For evaluator # 1/1 : <Environment.EvaluatorMultiPlayers.EvaluatorMultiPlayers object at 0x7fa7300c6860> (players Selfish-DiscountedThompson($\gamma=0.95$)) ...
  Last regrets vector (for all repetitions) is:
Min of    last regrets R_T = 2261.0
Mean of   last regrets R_T = 2391.08
Median of last regrets R_T = 2391.1000000000004
Max of    last regrets R_T = 2531.2
Variance  last regrets R_T = 3081.617599999988

Giving the mean and var running times ...

For players called 'Selfish-DiscountedThompson($\gamma=0.95$)' ...
    3.19 s ± 94.7 ms per loop (mean ± var. dev. of 40 run)

Giving the mean and var memory consumption ...

For players called 'Selfish-DiscountedThompson($\gamma=0.95$)' ...
    40 ± 3.4 KiB (mean ± var. dev. of 112.8 KiB runs)


- Plotting the decentralized rewards


- Plotting the centralized fairness (STD)


- Plotting the centralized regret
 -  For 2 players, Anandtharam et al. centralized lower-bound gave = 2.43 ...
 -  For 2 players, our lower bound gave = 4.86 ...
 -  For 2 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 2.62 ...

This MAB problem has: 
 - a [Lai & Robbins] complexity constant C(mu) = 1.36 for 1-player problem ... 
 - a Optimal Arm Identification factor H_OI(mu) = 33.33% ...
 - [Anandtharam et al] centralized lower-bound = 2.43,
 - [Anandkumar et al] decentralized lower-bound = 2.62
 - Our better (larger) decentralized lower-bound = 4.86,


- Plotting the centralized regret
 -  For 2 players, Anandtharam et al. centralized lower-bound gave = 2.43 ...
 -  For 2 players, our lower bound gave = 4.86 ...
 -  For 2 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 2.62 ...
 - [Anandtharam et al] centralized lower-bound = 2.43,
 - [Anandkumar et al] decentralized lower-bound = 2.62
 - Our better (larger) decentralized lower-bound = 4.86,


- Plotting the centralized regret
 -  For 2 players, Anandtharam et al. centralized lower-bound gave = 2.43 ...
 -  For 2 players, our lower bound gave = 4.86 ...
 -  For 2 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 2.62 ...
 - [Anandtharam et al] centralized lower-bound = 2.43,
 - [Anandkumar et al] decentralized lower-bound = 2.62
 - Our better (larger) decentralized lower-bound = 4.86,


- Plotting the centralized regret
 -  For 2 players, Anandtharam et al. centralized lower-bound gave = 2.43 ...
 -  For 2 players, our lower bound gave = 4.86 ...
 -  For 2 players, the initial lower bound in Theorem 6 from [Anandkumar et al., 2010] gave = 2.62 ...
 - [Anandtharam et al] centralized lower-bound = 2.43,
 - [Anandkumar et al] decentralized lower-bound = 2.62
 - Our better (larger) decentralized lower-bound = 4.86,


- Plotting the cumulative number of switches

- Plotting the probability of picking the best arm

- Plotting the histograms of regrets

- Plotting the cumulated total nb of collision as a function of time
No upper bound for the non-cumulated number of collisions...

- Plotting the frequency of collision in each arm
  - For #$0$: $B(0.3)$ ($8.7%$$\%$),	frequency of collisions is 0.08706  ...
  - For #$1$: $B(0.5)$ ($3.2%$$\%$),	frequency of collisions is 0.032155  ...
  - For #$2$: $B(0.9)$ ($4.4%$$\%$),	frequency of collisions is 0.04443  ...

- Plotting the frequency of collision in each arm
  - For #$0$: $B(0.3)$ ($8.7%$$\%$),	frequency of collisions is 0.08706  ...
  - For #$1$: $B(0.5)$ ($3.2%$$\%$),	frequency of collisions is 0.032155  ...
  - For #$2$: $B(0.9)$ ($4.4%$$\%$),	frequency of collisions is 0.04443  ...
Done for simulations main_multiplayers.py ...
